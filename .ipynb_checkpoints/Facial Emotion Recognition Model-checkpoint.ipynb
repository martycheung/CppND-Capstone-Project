{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "A quick model to classify emotion in faces. This model is intended to be used for the Capstone Project in the Udacity C++ Nanodegree. The idea is first to build a working model using Keras/Tensorflow in Python, without worrying too much about optimising performance since the aim of this project is to showcase C++ programming. Then we will save the model checkpoints and deploy it for model inference using C++. \n",
    "\n",
    "## Model and Data Summary\n",
    "- Data: Facial Emotion Recognition Dataset (FER) https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data\n",
    "- Model Architecture: MobileNetV2 (lightweight for real-time)\n",
    "- Pre-trained Model using ImageNet weights, fine-tuned to FER dataset  \n",
    "\n",
    "**Note:** Model building process is very similar to this face mask detector transfer learning tutorial https://www.pyimagesearch.com/2020/05/04/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/, but just applying to the new dataset. However I am not using the Python API of OpenCV, I will be saving the model checkpoints and then deploying using the OpenCV and Tensorflow C++ APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, Dropout, Flatten, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>Usage</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Training</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Training</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Training</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion     Usage                                             pixels\n",
       "0        0  Training  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
       "1        0  Training  151 150 147 155 148 133 111 140 170 174 182 15...\n",
       "2        2  Training  231 212 156 164 174 138 161 173 182 200 106 38...\n",
       "3        4  Training  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
       "4        6  Training  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in csv of pixel vectors, then convert to 48x48 numpy arrays\n",
    "# Also make np array of labels\n",
    "raw_data = pd.read_csv(\"data/icml_face_data.csv\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_np = raw_data.values\n",
    "raw_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array([np.reshape([int(s) for s in pic.split(' ')], (48,48)) for pic in raw_np[:,2]])\n",
    "labels = raw_np[:,0].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c6c9f1d0>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2dbaxfZZnur5u+AVYope/dtW9WBUTaWBS0AgEM6IzgBzWj44RJiHzwnMTJzHHEc5KTM8k5Ub+MfDjHOSFHM1Ung/MWIWTGY+VUJhNNsdAWcRraIi203W0pfaOi2O4+58P+b9J1Pdfe/3v/2/73ruv6JaR9Hp611rOetZ6ufV/7folSCowxv/tcMtETMMb0B292Y1qCN7sxLcGb3ZiW4M1uTEvwZjemJZzTZo+IuyPi+YjYFREPnq9JGWPOP9Hr79kjYgqAHQA+DGAvgJ8B+HQp5d9HO2b69Onl8ssvb/Rdcknz35uIqI4bGhoasz2OOTfaZ86c6TpGwdd/y1veUo2ZPn161cf3qtae+9QYvr66D9XXbT7cBoApU6ZUfVOnTm20p02bdt7O3Qu8Rq+//no15tSpU422es5qzXitT58+3XWMemaZ55F5P7mPr3Xq1CkMDQ3Jl3iq6kzyPgC7Sim/7Ez0EQD3Ahh1s19++eVYt25do++tb31ro61egNdee63RPnLkSNfJqfPwC6deCn5x1XmOHTvWaN94443VmOXLl1d9l156aaOt/tHil/K3v/1t1+v/+te/rsaovm7zmTlzZjXmiiuuqPrmzJnTaM+bN68aw891xowZ1ZhZs2Y12url5n9Y1GbjNdu2bVs15sCBA2OeFwB+85vfVH3Hjx9vtF955ZVqzNGjR8ecD1A/D/Ve8bt38uTJagz38Zrt2bOnOmaEc/kxfjGAl89q7+30GWMmIeey2dWPCtXPLxHxQERsjojN6itljOkP57LZ9wJYclZ7AMB+HlRKebiUsraUslbZscaY/nAuNvvPAKyKiOUA9gH4AwCfGeuAM2fOVDYo21LKJmMRQtm6V111VaPN9ihQ27/KbmN9QNlfH/nIRxptZbOzEAnoe2My/yC+8cYb4z6vEsjYRlRrpubDtr0aw30ZwVLBtq169qy9rFmzphqzcePGRnvfvn3VGPXMeI7q+rxu6p1hlEDIdr16P3nt+d7HEph73uyllNMR8R8B/F8AUwB8q5Tyi17PZ4y5sJzLlx2llH8G8M/naS7GmAuIPeiMaQnn9GUfL0NDQ3j11VcbfWxzXHnlldVx/DtbZTexfaN+Z8q2FP++GgBmz57daN95553VmNWrVzfayh7t1WGE7Whlg/G5lVMLH5c5T8auV33KtuQ1UevBc1LOKDxG6RN8fX5fAGDVqlWN9vPPP1+NUffKdjT/3h2o32G1Hnxvyg+C9SKlIbA+wNcay2b3l92YluDNbkxL8GY3piV4sxvTEvoq0J0+fboSxVhwUKIEO/+rAAEWNzKC0Lve9a5qzF133dVoL1iwoBrDQlZWoONxKvCDj8sETCj4XpX4xuJOZs0UmYg2JVrx/Svhlc+jAmr43Erou+aaaxrtH/7wh9UYJdiyE4sS1niN1H2wsKje4Yw7ebc1GyuK1V92Y1qCN7sxLcGb3ZiW0FebHahtDrZdVEIJdpBRCRXYblRjrr/++kb7lltuqcbMnz9/zPMqlI2m7Hi+94ytm3HOySTqyByn7POMPqCuz2uibHaeoxqTga+vHG/mzp3baC9atKga8/TTT1d9ixc3UzQohx1OXqECijLwcZzcA6htctaqxsJfdmNagje7MS3Bm92YluDNbkxL6HvUGzsusJOEEpauvvrqRpvFFqAW5FauXFmNueGGGxptlQKaUeJTr+m3+d4yqYLVerD4pYQ1XteJLs2tHEYy6aYzQmOGyy67rNFesWJFNeZHP/pR1ccC2NKlS6sxJ06caLQzEYbKOYiF6MOHD1djOCo0k6J6BH/ZjWkJ3uzGtARvdmNaQt+dahi2XTLVRZSzwTve8Y5Ge9myZdUYttt6Lb/ENplyBlF2WyY4JXMeJmOPZ66VcXxRc1JzHE8GlRGU/Zl5Hr2gKvao7EaDg4ONttJ5WC9Sji4ZByLOkMxZnYA6EGc86+MvuzEtwZvdmJbgzW5MS/BmN6Yl9F2gY+cCTu+cSRWsnGrY8UY5LWRKCbFIk4kEy4hPQH1vSky5UPXwMnXeM4Jl9rhMaaleotyUkxOvqxL6eIzKQKTmyCWa1bkz5Zgzc+Q1U0I0n5vFQKeSNsZ4sxvTFrzZjWkJfbXZSymVjc52irJl2LZTJaLY1lXOIGxbZoJMlKMFj1HZXDLOKEqf4D5l17LdlnFGUXPkMZmAGqC+/0w5LpWVlVFrlgmU4jGqZDIH4ij7XGWh2b9/f6Ot7Gh+RzJ6UcbJSc2R1zGjA715za5XNMb8TuDNbkxL8GY3piV4sxvTEvruVJMRtxiOKlKRR9ynHBtYyFICWS+pozMimhqnBDElLjGZVMXdymwB9b1mnGNU3xtvvFGN4XtTZb0y4lKmRBSjnkdmXdWz/9WvftVoK8E28zx4zTLzUfD1nanGGFPhzW5MS+i62SPiWxFxKCKeO6tvdkRsiIidnT+vGuscxpiJJ2Oz/zWA/wng22f1PQjgiVLKVyPiwU77S91OFBGVfcMOCLNnz66O4z7O6AHkgioyWT0yjiaZLLHKJsvYVzxGZT3hElnKGYTvg0sUAfXaK9tbrRGXyFLXZyeWbLAQkymHxXa80mK6nRfQGXAzZZQz12ObXWkhmUCtblmDzykQppTyrwCOUPe9ANZ3/r4ewMe7nccYM7H0arPPL6UMAkDnzzpxnDFmUnHBf/UWEQ8AeAA4fwn/jTHjp9fddzAiFgJA589Dow0spTxcSllbSlnbq91mjDl3ev2yPwbgPgBf7fz5aOagSy65pBLoOMPMqlWrquMGBgYabeWgkaljzoJH5hgl0LAgxSmqRzs333vmJx0lkHHkkxLW+NwZQUiNYTEQqB071L2ePHmy0VYZeHgd1VrztZQYlimZxccpAVXdKwu/ao34+avzMOrDl4l47BYlek4CXUT8LYCfAnhnROyNiPsxvMk/HBE7AXy40zbGTGK6ftlLKZ8e5X/dcZ7nYoy5gFgxM6Yl9DUQZurUqZWNzmWaVClddqrpNVMp20SZQBhlj7JdpJwfVB/bV8rWz5SIUpl6GLZJVfYUdX1GlSA6fvx4o600Az5OrQc/R+Wcw8cpW7sXvebIEXYdqe9LzVHZ4/yMei3ZxXNUOke3bEvOLmuM8WY3pi14sxvTErzZjWkJfRXopk2bVpXdWbp0aaOtSjuxUJGpI64izDJpolkAUSLJwYMHG212IAF06mQWl5SYMmfOnEZbORCxQMeZfIB63uo8fFw2JTZH4p04caIaw31qjvv27Wu0WbwFgJkzZzbaGQcm9Vy7lU0C9Bqxo486N6+bej/5vcpk3MmIeOPBX3ZjWoI3uzEtwZvdmJbgzW5MS5hwDzpOMaU8vTLpplkUUQIde3opbywW25TQxt5hyvNKCXsZz7vDhw93HZOpyc3rwZGDQC2GKg82VduMr6eEJBbElOcZe7FlUk6pZ8YinpoPz1k9HyXQ8TujIvN68d7MzDHDeGrc+8tuTEvwZjemJXizG9MS+mqzX3LJJVWZJnaSUE4cbMtlbJtMJJRykGD7W9mameg1ZWuzvansTy7bxOWHgNquz5Qk2rt3b9cxyqlFpe1esmRJ1zGsB6h7Zccb5ZzEGo6ymdn+ZhseyOk+qqwYp+BWkZKZDDNj1U0fz5huUZGOejPGeLMb0xa82Y1pCd7sxrSEvgp0EVEJJSwoZGprKYcIFkWU0JeJTmLRSolfLJC9+OKL1RgllGTqrzFKIMzUA2fxSwlLHD2n7lUJhBzBppxxWMRk4REABgcHG22V3ooF05dffrkaw+uq5sNORerZq75exOCMk5OCRV11Hn72mTpzI/jLbkxL8GY3piV4sxvTEvpqs5dSKueKTIBAprwP201qDNtymfMo5xh2GFF2k3LQYFtfObrwvarAoNtvv73RVppBJpUz96mgF3UfrJkopyI+bvfu3dWYH/zgB422SlvNmoV6ZmwPr1mzput81Jqpd4/PrZyDeB3VHDPOL/zsM042mf3z5hy6ns0Y8zuBN7sxLcGb3ZiW4M1uTEuYcIGOBRgVQZURN1hIUcIWixeZ+tec2hmoBak9e/ZUY5RzULfa2gDw0ksvNdo333xzNWbdunWNtnIyYgFKCW38LBYvXlyN4Tp7ALBw4cJGe968edUYXrfvf//71ZitW7c22iorEIuht912WzWGOXToUNXHQpaq9ZbJuKMEOj53JsIuE5Wp5jMeJxrGX3ZjWoI3uzEtwZvdmJbQV5t9aGioCtDg4Adlx2ZqpmccPdhGV44m7Hij7Ei2pRYtWlSNee6556o+ntMnP/nJagxfT2WP4XkvX768GsOBH6qmOzv5ZJxj1JyUrc92vNIVOOONCpZhVq5cWfWtXr260X7yySerMWxrZ7LiALU+lLG1e7XHeY0yjjfjyeLkL7sxLcGb3ZiW4M1uTEvoutkjYklEbIyI7RHxi4j4Qqd/dkRsiIidnT/rX5AbYyYNGYHuNIA/K6U8ExFvBfB0RGwA8McAniilfDUiHgTwIIAvjXWioaGhqnRRprxPJhqIhQnlMMMCTC8CCFBndFGZUa677rqqjyO/OFMLUKdBVpFg7LShxDcW1pT4xA5Mal3Vcexok6n9fvfdd1dj2PFGOcPwc7z11lurMcxdd91V9e3YsaPRVll5lBiZiYLsBfXu8XPNvOfjKRnV9cteShkspTzT+ftrALYDWAzgXgDrO8PWA/h4+qrGmL4zLps9IpYBWANgE4D5pZRBYPgfBAC1z+TwMQ9ExOaI2Ky+2saY/pDe7BExE8A/AviTUsqJbuNHKKU8XEpZW0pZm/EZNsZcGFJONRExDcMb/W9KKf/U6T4YEQtLKYMRsRBAbXARp06dwv79+xt9nL00U7ZXOd6wfaPG8LnVPz48Rtms7JCh7DgV0MPON6xfAHWgQ8bJSGVGYftTaRjswKOyu6r7Z+ebTJnta6+9tuv1VVlp1jBURl6ej3Iy2rJlS6Od0YaAnE3cS+ZYBc9JvZ88n/Fkt8mo8QHgmwC2l1L+8qz/9RiA+zp/vw/Ao93OZYyZODJf9g8C+CMAP4+IkZjE/wzgqwD+LiLuB/ASgNr30xgzaei62Usp/wZgtJ9l7ji/0zHGXCjsQWdMS+hr1Nv06dMrEYbFFSUwsHCkorN4TEZ8U8IKi13KYYTHqOwh6vospihnGBZpMsKOKgfF11Jz5HMrcVSdm7PwqDnyGrGTDQCsWLGi0d63b181hqMkFeyco8TRTDks5WjTSwRbRtTLiMyZa7v8kzGmwpvdmJbgzW5MS+irzT5jxowq0wjb38puYrsxk2VE2TLsxKFsIrabMuWh1bWU0wafO1NKKFNaOGMjqjlmgjwyc1SZdDP3wQFECxYsqMYsW7as0Vb3mtFi+P7VeTL2r9KUegmOUcfwnNSYTJmz0fCX3ZiW4M1uTEvwZjemJXizG9MS+l7+iUUQFuRU6uaMAMLHKZEmcx52dlCRYOxoo6K+lNMEi12ZbCUZMg4zSvjMRBMqgY7vV62rEu0YFj9Vxh+OilSCKYu8ag15jZQ4q9YxkxmGj1NrlomM4/VXx/C6Zmq4j+AvuzEtwZvdmJbgzW5MS/BmN6Yl9FWgi4hK3Ml4lTFKEMt4x2Ui7DKCB4tGKjJOiUQsLqn7YDKikYK9DFUKrG4pjka7FgtH6pnx81CRioxaM15b5THG66oEQ56zEr/U/bNAm0k/rt6h8Xi6jXWeXmrPvTmHcc/AGHNR4s1uTEvwZjemJfTdZu/mNKLsLbaJlN3Edpuyddn+UnYb25/K1uJzq2upUkJ8H5lyQwq+3uuvv16NOX78+JhtIFfGSvXxuqlsNnwfyhmm23lH62P4GSkNIRP1pq6lnKoYZet3IxPRlhnD+8n12Y0x3uzGtAVvdmNagje7MS2hrwKdggWFTFpmFVHFThxKNOJrZcQXJdqw2KSu1Wv9r0zNum6RgwBw7NixRpsdgYC6znrGqQWoHWSU+MZ9agyvm7oPnpN6Znxv6v1gJ6OsGMiOTyolWsapp5c00WpML6nGR/CX3ZiW4M1uTEvwZjemJfTdqYYdIDIOCWyXKDuFbXau6w3UtpXKisP2sHJ8Yds/U0ZKjVO2Jdtpan34OBXkwjY72+eqT11L3Rvff8bxSD0zXltlD/eStvuVV16pxhw+fHjM844G2+y9lINSZDLeqGvxvJ2pxhhT4c1uTEvwZjemJXizG9MS+u5U0y0aKpOWuVdHE3bQ6LWuOYsrGaEPyDmaZM7DTiSvvvpqNYZFK+Uww0Kfmk8mgkytNYt2SnxTImq362dqpL344ovVGI4MVPeq3itet8w6ZtYsIxBmMhJlUl2P4C+7MS3Bm92YltB1s0fEpRHxVERsi4hfRMRfdPqXR8SmiNgZEd+LiO4/kxpjJoyMzf4GgNtLKScjYhqAf4uIfwHwpwC+Xkp5JCL+N4D7AfzVWCdSTjUZJxt2vsjYyJk678pu4sCPjI04HseG8R6nHG/YZlc2Ijva7Nu3rxrD63j06NFqjFpHzl5zzTXXVGPmzZvXaKtAHNZnFi5cWI1hu14FwvA6bt++vRrD70dGLwFyelFG58mUw2J6fa9Go+uXvQwzoq5M6/xXANwO4B86/esBfPy8zswYc15J2ewRMSUitgI4BGADgBcAHCuljPwTthfA4gszRWPM+SC12UspQ6WU1QAGALwPQP1z2/DXviIiHoiIzRGxWSVGNMb0h3Gp8aWUYwB+DOAmALMiYsToHQCwf5RjHi6lrC2lrFWJEIwx/aGrQBcRcwGcKqUci4jLANwJ4GsANgL4BIBHANwH4NHMBdmpJlOCiAUP5djAIpUSRFhsytRwz0R0KZRzA58rU94nU1ddRbTt2LGj0f7JT35Sjdm9e3ejfeDAgWqMmuPAwECjvW3btmoM36sSxG699dYxzwvknGo4ym3nzp3VGH5n1HuWeR6ZaEb17MeTUWasY86l/FNGjV8IYH1ETMHwTwJ/V0p5PCL+HcAjEfHfAWwB8M30VY0xfafrZi+lPAtgjej/JYbtd2PMRYA96IxpCX0NhCmlVDYoO7ooeydTkilDpmwTX0sF5rBtp2w9ZW9xnzouU/6XnVqU/bly5cpGW2VvYVvz7W9/ezVGOcNwFhzl5LRs2bJG+0Mf+lA1hm30zPNQ68pONDw/oL5XpYUo+5dtfTWG1z9TMrqXklFA7+8+4C+7Ma3Bm92YluDNbkxL8GY3piX0XaDLpBhmukXKAeNzLhghE52USeerRMWMaJdx7FACIZdfUmu4eHEzVOG2226rxrBop4Q2laaa0zIrp55169Y12ioyjiPx1PPg9VeC4bPPPlv1MRlHqAwZhxk1hvt6rc9ugc4Y0xVvdmNagje7MS2hrzb7mTNnKpuLbRBVbimTKZVRtm4mWwjb48pGymSqyZT3yVxf2fWcrUXZdhlthG3/K664ohozf/78qo+z0KjjeN4qmw5fP1Ou+/nnn6/G7Nq1q9HOZA3OPp+MU00v5ZgV/IyyJaqy+MtuTEvwZjemJXizG9MSvNmNaQl9L//EZCLRWPDota55Jm01O5FkzqOEFCUisgCj0iKzsHjixIlqDDu/KBGR563y//F81LqqDDN8v+r+OapMlajiNNHq+nyvKuNORvhUAiHTa+rmTGkp7lNzzjhrMfzsXf7JGOPNbkxb8GY3piX03amG7VS2OZRtxXZsptyuOg9fK+OMomxvno+yv5QdzXNSx/H6HD9+vBrDtmWvgRdsM6t1VXY03xs7xwC1/XnkyJGuY1Sq8U2bNjXanBFXXV9pIbz2qly0Oo5Zvnx51cdBPnv27KnGvPTSS402ZxsC6vcqoxeNB3/ZjWkJ3uzGtARvdmNagje7MS2h75lqVDaUbrBQoUQK7lOpgvk8KjIuU36Jo6oyKaGBWiRTji6Z9clk7mFhUQmWJ0+ebLRVtNiiRYuqvoz4lxEReYwSLH/605822uo+WOjLRKbxvQM6lfZ73vOeRnvu3LnVmM985jNjXgsANmzY0Gh/4xvfqMYMDg422r1EvY2VotpfdmNagje7MS3Bm92YluDNbkxL6KtANzQ0JFMBn40SidizSQkwmbrq7CGWSSelRCMW0TL14NQc1bk5TbOq0caReSoyjuudKWGLr68EoVWrVlV9vLbK8+zmm29utK+++upqDPft3bu3GsMec8pbj+9D3euCBQsa7RtuuKEa87nPfa7q4zTV3/nOd6oxnDZbpem66aabGm2Vfvuhhx5qtNVz5XeYBeSxUmL5y25MS/BmN6YleLMb0xL6HvXG9mbG3uIIJVV/m50JMtlClF2fcWTgOao5K9uJ7SvlaLJw4cJGWzl/cFTV0aNHqzF8nHLg4T4Vdabs6FmzZjXaV155ZTWGnU9UinBeoy1btlRjWONR5+H341Of+lQ15pZbbhlzfoBeI35f1fvxla98pdE+cOBANYa1BnX9JUuWNNpcHkv1XXXVVY32WFFx/rIb0xK82Y1pCenNHhFTImJLRDzeaS+PiE0RsTMivhcR9c/NxphJw3i+7F8AsP2s9tcAfL2UsgrAUQD3n8+JGWPOLymBLiIGAPwegP8B4E9jWFm6HcBIuM96AP8NwF+NdR5Vn51FESWA8JhM/TWVzonHqAghFvGUiMZjlFNJpt6XSk3EDhkqEkvVWmd4jZQzEwudau2VgwgLcpmoN3V9FhG3bdtWjeHoPfXM7rjjjkb7/e9/fzWG7/Wxxx6rxqjrHzp0qNFWabpYxFNps1nUVO8ni4/q/eB13bFjR6OtojRHyH7ZHwLw5wBGVvpqAMdKKSM7dy+AxclzGWMmgK6bPSJ+H8ChUsrTZ3eLoTLDfkQ8EBGbI2JzJlG/MebCkPkx/oMA7omIjwK4FMAVGP7Sz4qIqZ2v+wCA/ergUsrDAB4GgJkzZ/ZWcsMYc8503eyllC8D+DIARMRtAP5TKeUPI+LvAXwCwCMA7gPwaOJclQMK2zvK/mUHhEz9bYVytGHYblXHsD2eyWYD1A4QKk01r4/KXMNzVEE37MTBDhtAHeSiAnrUunYLZgLqdVOOP2xvqtrrfK/XX399NWbFihVdr7V9+/ZGWzkLKVubn60KYGGUzsPveSYIi+18ALjnnnsabXbg+fa3vz3qvM7l9+xfwrBYtwvDNvw3z+FcxpgLzLjcZUspPwbw487ffwngfed/SsaYC4E96IxpCd7sxrSECa/PzqiaYOwgooQLFjyUsMbRUZlfBSpHExZblBiXqfXG51Fk6ngrBx7OcrJ/f/3LEhb/1HnUWs+ZM6fRVtljWNhSz5XTRM+fP7/r9W+88cZqDDufqPtQDjPMtddeW/Xt2rWr0VbiJGfBydTnU5GbLE6rZ3/dddc12ux0pUTWEfxlN6YleLMb0xK82Y1pCX3PVMM2D9sYytZle0+NYbtZOaPwcSrIg219lb0kk/FGZVThOakAkozDDjuNKCcSDthQARyc9UTZkSq77Ac+8IFGW9navCZPPfVUNeaZZ55ptN/97ndXYz72sY812mwfA/WaqWfP9dGVXsNOT0Bd/kplfOUMwGo9WC9ihyKgftbqWk8++WSjzQ5F5yMQxhhzkePNbkxL8GY3piV4sxvTEvoq0M2YMQPvfOc7G33s7KDSGbMApZwN5s2b12gr8SuTtpodRJSDBjtxqOi1TErqTF11dR/sfKGcODJlgfhes2WsVKpkhtf68ccfr8awaPb5z3++GsNRbkqg4zV64YUXqjH8rJWT0cGDB6s+vh4LYkD9zNR5li1b1mir9Nsc9afeIX7WW7dubbSVODmCv+zGtARvdmNagje7MS2hrzb7tGnTKocDdtpgOw6obUkVVMHOOsomYmcYZcdythAV5ML2n3JYUbYTO/Goc/MYdsYA6uAQ1kGAXFZWvg+lIWSy5CpdYePGjY32yy+/XI354he/2Gh/9rOf7TpHlQWGy1yrwJzly5c32lyKGdBZe7n0NJfnAmpnJFXGiu9/6dKl1Rh2KlLaA+tXmdLcI/jLbkxL8GY3piV4sxvTErzZjWkJfRXohoaGKnGNnWFUql4uE6RS/rKYMTg4WI1hoU9FtLGwpoS+TNpq5aDC11NjWFxSEVQsvinHF3b8UXNmYU0JbUrw4Qw7SuzatGlTo60cRFjsUs46HD2onv3u3bu7juEsNCtXrqzGqKg3duoZGBioxmRSa7P4qSIMWXhVmXP4GfHeGHMO6ZHGmIsab3ZjWoI3uzEtYcKzy7L9p2xktuu5DdROE5w9BKgzhaoxHGSisoWwM4xytFD2ONvRyjmInUZUNh0eo+xqtpGVMwo7A6kxqhwX24k///nPqzFcXknZlt/97ncbbfXs3/a2tzXaal0zZbb5GSkNQ70P/IxUBiK2xzOlwFUpbs4cpDIkv/e972202c5XAT5vznPU/2OM+Z3Cm92YluDNbkxL8GY3piWEcsi4YBeLeAXAHgBzABzuMnyycTHOGbg45+05987SUspc9T/6utnfvGjE5lLK2r5f+By4GOcMXJzz9pwvDP4x3piW4M1uTEuYqM3+8ARd91y4GOcMXJzz9pwvABNisxtj+o9/jDemJfR9s0fE3RHxfETsiogH+339DBHxrYg4FBHPndU3OyI2RMTOzp918PMEEhFLImJjRGyPiF9ExBc6/ZN23hFxaUQ8FRHbOnP+i07/8ojY1Jnz9yKidhKfYCJiSkRsiYjHO+1JP+e+bvaImALgfwH4CIBrAXw6IuoI/YnnrwHcTX0PAniilLIKwBOd9mTiNIA/K6VcA+AmAP+hs7aTed5vALi9lHIDgNUA7o6ImwB8DcDXO3M+CuD+CZzjaHwBwPaz2pN+zv3+sr8PwK5Syi9LKb8F8AiAe/s8h66UUv4VAIek3Qtgfefv6wF8vK+T6kIpZbCU8kzn769h+EVcjEk87zLMSDjctM5/BcDtAP6h0z+p5gwAETEA4PcA/J9OOzDJ5wz0f7MvBnB2AtC5kXAAAAHCSURBVO29nb6LgfmllEFgeGMBqONsJwkRsQzAGgCbMMnn3flxeCuAQwA2AHgBwLFSykixuMn4jjwE4M8BjMSyXo3JP+e+b/Y60Hj4X3JznoiImQD+EcCflFLqYPxJRillqJSyGsAAhn/yu0YN6++sRicifh/AoVLK02d3i6GTZs4j9Dt5xV4AS85qDwAYPdp+cnEwIhaWUgYjYiGGv0STioiYhuGN/jellH/qdE/6eQNAKeVYRPwYw3rDrIiY2vlSTrZ35IMA7omIjwK4FMAVGP7ST+Y5A+j/l/1nAFZ1lMvpAP4AwGN9nkOvPAbgvs7f7wPw6ATOpaJjN34TwPZSyl+e9b8m7bwjYm5EzOr8/TIAd2JYa9gI4BOdYZNqzqWUL5dSBkopyzD8/v6/UsofYhLP+U1KKX39D8BHAezAsG32X/p9/eQc/xbAIIBTGP5p5H4M22VPANjZ+XP2RM+T5rwOwz86Pgtga+e/j07meQN4D4AtnTk/B+C/dvpXAHgKwC4Afw9gxkTPdZT53wbg8YtlzvagM6Yl2IPOmJbgzW5MS/BmN6YleLMb0xK82Y1pCd7sxrQEb3ZjWoI3uzEt4f8DI3yCAfOx60sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perform one-hot encoding on the labels\n",
    "# lb = LabelBinarizer()\n",
    "# labels = lb.fit_transform(labels)\n",
    "# labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train, Test, Val\n",
    "train_images = np.stack((images[raw_np[:,1]==\"Training\"],)*3, axis=-1)\n",
    "valid_images = np.stack((images[raw_np[:,1]==\"PublicTest\"],)*3, axis=-1)\n",
    "test_images = np.stack((images[raw_np[:,1]==\"PrivateTest\"],)*3, axis=-1)\n",
    "\n",
    "train_labels = labels[raw_np[:,1]==\"Training\"]\n",
    "valid_labels = labels[raw_np[:,1]==\"PublicTest\"]\n",
    "test_labels = labels[raw_np[:,1]==\"PrivateTest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 4, 0, 4])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 48, 48, 3)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained ImageNet Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(48, 48, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(48, 48, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "outputs = Dense(7, activation='softmax')(x)\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Model)             (None, 2, 2, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_6 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 14343     \n",
      "=================================================================\n",
      "Total params: 23,602,055\n",
      "Trainable params: 14,343\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 897 steps, validate on 3589 samples\n",
      "Epoch 1/20\n",
      "340/897 [==========>...................] - ETA: 3:01 - loss: 1.7637 - accuracy: 0.3355"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-201-3d6e19cd9817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           epochs=EPOCHS)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Will do training on Google Colab GPU instance\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(optimizer=opt,\n",
    "              loss=\"sparse_categorical_crossentropy\", # sparse means the label input was just a vector of classes from 0-6\n",
    "              metrics=[\"accuracy\"])\n",
    "hitsory = model.fit(datagen.flow(train_images, train_labels, batch_size=BATCH_SIZE), \n",
    "          steps_per_epoch=len(train_images) // BATCH_SIZE,\n",
    "          validation_data=(valid_images, valid_labels),\n",
    "          validation_steps=len(valid_images) // BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          callbacks=[EarlyStopping(monitor='val_accuracy', mode='max', patience=5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predIdxs = model.predict(test_images, batch_size=BS)\n",
    "print(classification_report(test_labels, np.argmax(predIdxs, axis=1), target_names=lb.classes_))\n",
    "model.save(args[\"model\"], save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(args[\"plot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/Users/martincheung/Desktop/Learning Resources/C++ Nanodegree/CppND-Facial-Emotion-Recognition/model' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1234",
   "language": "python",
   "name": "test1234"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
